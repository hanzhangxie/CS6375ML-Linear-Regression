import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# split the dataset into training/testing sets
x = np.array([116.0, 68.0, 111.0, 68.0, 88.0, 68.0, 82.0, 134.0, 58.0, 95.0, 143.0, 62.0, 68.0, 69.0, 69.0, 68.0, 160.0, 112.0, 116.0, 110.0, 84.0, 88.0, 101.0, 156.0, 155.0, 69.0, 68.0, 92.0, 69.0, 114.0, 145.0, 161.0, 97.0, 88.0, 52.0, 56.0, 84.0, 86.0, 123.0, 110.0, 62.0, 114.0, 94.0, 68.0, 94.0, 95.0, 100.0, 95.0, 92.0, 56.0, 102.0, 70.0, 69.0, 123.0, 82.0, 68.0, 162.0, 76.0, 68.0, 62.0, 88.0, 152.0, 73.0, 121.0, 200.0, 60.0, 110.0, 85.0, 110.0, 97.0, 102.0, 52.0, 121.0, 82.0, 102.0, 90.0, 97.0, 69.0, 88.0, 70.0, 68.0, 70.0, 123.0, 69.0, 160.0, 110.0, 162.0, 92.0, 68.0, 92.0, 161.0, 116.0, 70.0, 68.0, 176.0, 106.0, 84.0, 142.0, 114.0, 68.0, 86.0, 82.0, 112.0, 76.0, 116.0, 70.0, 76.0, 111.0, 85.0, 62.0, 76.0, 69.0, 68.0, 84.0, 160.0, 62.0, 95.0, 102.0, 86.0, 160.0, 120.0, 76.0, 140.0, 116.0, 68.0, 114.0, 116.0, 152.0, 101.0, 70.0, 70.0, 85.0, 48.0, 55.0])
y = np.array([0.9959, 0.5572, 1.1694, 0.6189, 0.8189000000000001, 0.6229, 0.8013, 2.1485000000000003, 0.6479, 1.32, 2.2018, 0.6918000000000001, 0.5572, 0.5499, 0.7349, 0.5195000000000001, 1.8150000000000002, 0.9298000000000001, 0.9279000000000001, 1.5510000000000002, 1.1245, 0.8921, 1.6925000000000001, 1.5690000000000002, 3.5056000000000003, 0.6649, 0.6229, 1.1248, 0.5118, 1.6515000000000002, 1.2964, 1.5998, 0.8949, 0.8499, 0.7775000000000001, 0.7788, 0.8495, 0.8845000000000001, 2.5552, 1.504, 0.8778, 2.2625, 0.996, 0.6377, 1.0198, 1.7950000000000002, 0.9995, 1.558, 0.8948, 0.7898000000000001, 0.7957000000000001, 0.8238000000000001, 0.8249000000000001, 2.8248, 0.7463000000000001, 0.6695, 1.895, 0.6855, 0.9495, 0.6338, 0.8921, 1.4399000000000002, 0.7053, 2.1105, 1.9699, 0.5399, 1.185, 0.8495, 1.7710000000000001, 0.9549000000000001, 0.7957000000000001, 0.7995, 2.097, 0.7775000000000001, 1.395, 0.998, 1.19, 0.7799, 0.6989000000000001, 0.7198, 0.7609, 0.7738, 2.8176, 0.6849000000000001, 1.7199, 1.217, 1.842, 1.0898, 0.6692, 0.9988, 1.6558000000000002, 0.9989, 0.6575000000000001, 0.6095, 3.225, 2.247, 0.8845000000000001, 1.8150000000000002, 1.294, 0.7395, 0.9095000000000001, 0.7126, 0.9538000000000001, 0.7129000000000001, 0.8449000000000001, 0.6295000000000001, 0.6529, 1.1259000000000001, 0.8195, 0.6488, 0.7295, 0.7299, 0.6692, 1.0245, 1.9045, 0.5348, 1.6900000000000002, 0.7689, 0.7895000000000001, 1.8399, 1.828, 0.7295, 2.3875, 0.9279000000000001, 0.5389, 1.3415000000000001, 1.7669000000000001, 1.3499, 1.2945, 0.9258000000000001, 0.6938000000000001, 0.7975, 0.5151, 0.7099000000000001])

x_test=np.array([100.0, 69.0, 62.0, 114.0, 97.0, 70.0, 68.0, 82.0, 116.0, 73.0, 116.0, 68.0, 152.0, 115.0, 73.0, 69.0, 70.0, 84.0, 101.0, 123.0, 68.0, 160.0, 116.0, 114.0, 86.0])
y_test=np.array([1.0345, 0.7499, 0.7898000000000001, 1.6845, 1.663, 0.8058000000000001, 0.6795, 0.9233, 1.1199000000000001, 0.7603000000000001, 0.9639000000000001, 0.7609, 1.3499, 1.745, 1.0698, 0.7999, 0.8358, 1.0595, 1.643, 3.16, 0.6669, 1.862, 1.1549, 1.5985, 1.0295])

n = len(x)

# fit LinearRegression models
lr = LinearRegression()
# train the model using the training set
lr.fit(x[:, np.newaxis], y)
# make predictions using the testing set
y1 = lr.predict(x_test[:, np.newaxis])

# the coefficients
print('Coefficients: \n', lr.coef_)
# the mean squared error
print("Mean squared error: %.2f"
      % mean_squared_error(y_test, y1))
# explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(y_test, y1))

# plot outputs
segments = [[[i, y[i]]] for i in range(n)]
lc = LineCollection(segments, zorder=0)
lc.set_array(np.ones(len(y)))
lc.set_linewidths(np.full(n, 0.5))
fig = plt.figure()
plt.plot(x_test, y_test, 'r.', markersize=12)
plt.plot(x_test, y1, 'b-')
plt.gca().add_collection(lc)
plt.legend(('Data','Linear Fit'), loc='lower right')
plt.title('horsepower-price regression')
plt.xlabel('horsepower')
plt.ylabel('price')
plt.show()